{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting pytorch-lightning\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/58/01/5df6324efdc3f79025ea7eaf19478936c401a16dae4fd3fbd29f7d426974/pytorch_lightning-1.2.6-py3-none-any.whl (829kB)\n",
      "\u001b[K     |████████████████████████████████| 839kB 1.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sklearn in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pytorch-lightning) (1.4.0)\n",
      "Collecting fsspec[http]>=0.8.1 (from pytorch-lightning)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/62/11/f7689b996f85e45f718745c899f6747ee5edb4878cadac0a41ab146828fa/fsspec-0.9.0-py3-none-any.whl (107kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 29.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchmetrics>=0.2.0 (from pytorch-lightning)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/3a/42/d984612cabf005a265aa99c8d4ab2958e37b753aafb12f31c81df38751c8/torchmetrics-0.2.0-py3-none-any.whl (176kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 17.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML!=5.4.*,>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pytorch-lightning) (5.3.1)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pytorch-lightning) (2.3.0)\n",
      "Requirement already satisfied: future>=0.17.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pytorch-lightning) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pytorch-lightning) (1.19.1)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pytorch-lightning) (4.45.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from sklearn) (0.20.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (0.23)\n",
      "Collecting aiohttp; extra == \"http\" (from fsspec[http]>=0.8.1->pytorch-lightning)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/99/f5/90ede947a3ce2d6de1614799f5fea4e93c19b6520a59dc5d2f64123b032f/aiohttp-3.7.4.post0.tar.gz (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 16.6MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests; extra == \"http\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (2.22.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.26.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (41.4.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.10.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.33.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.16.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.8.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.7.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.12.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.5.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->fsspec[http]>=0.8.1->pytorch-lightning) (0.6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (19.2.0)\n",
      "Collecting typing-extensions>=3.6.5 (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (3.0.4)\n",
      "Collecting async-timeout<4.0,>=3.0 (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/1c/74/e8b46156f37ca56d10d895d4e8595aa2b344cff3c1fb3629ec97a8656ccb/multidict-5.1.0.tar.gz (53kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 41.4MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/97/e7/af7219a0fe240e8ef6bb555341a63c43045c21ab0392b4435e754b716fa1/yarl-1.6.3.tar.gz (176kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 62.7MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (1.25.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.7)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.0)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->fsspec[http]>=0.8.1->pytorch-lightning) (7.2.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
      "Building wheels for collected packages: aiohttp, multidict, yarl\n",
      "  Building wheel for aiohttp (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for aiohttp: filename=aiohttp-3.7.4.post0-cp37-cp37m-linux_x86_64.whl size=1145946 sha256=d195cd0fff726e2fae6b8014f1cd8d4a61ac8c65aaaddd60d0450e879d10fa1c\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/f9/ca/b6/2e86b8b659d65c6707a0c120226cea7b74cd4dcb0e108ca141\n",
      "  Building wheel for multidict (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for multidict: filename=multidict-5.1.0-cp37-cp37m-linux_x86_64.whl size=142492 sha256=7b370b10e0d51f0a4e555d8965f584ebfd50bf83f77a492339cabfa471f426de\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/45/c3/00/59f7cf8d9fbdc4df3b8dd08d1619f1e9756ef639468277d633\n",
      "  Building wheel for yarl (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for yarl: filename=yarl-1.6.3-cp37-cp37m-linux_x86_64.whl size=244176 sha256=2ae029ebf0bf573d125bc85794a37f2c1e22c3e49b92713a439026282db9ae1b\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/b0/60/d6/a6db77622a25b1d325ce0f38d8fd41c0fa7e9590b902f61a5e\n",
      "Successfully built aiohttp multidict yarl\n",
      "Installing collected packages: typing-extensions, async-timeout, multidict, yarl, aiohttp, fsspec, torchmetrics, pytorch-lightning\n",
      "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-0.9.0 multidict-5.1.0 pytorch-lightning-1.2.6 torchmetrics-0.2.0 typing-extensions-3.7.4.3 yarl-1.6.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install pytorch-lightning sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 118.3 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
    "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
    "  print('re-execute this cell.')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "x = torch.randn(100000, 2)\n",
    "noise = torch.randn(100000,)\n",
    "y = ((1.0*x[:,0]+2.0*x[:,1]+noise)>0).type(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "y_np = y.numpy()\n",
    "x_np = x.numpy()\n",
    "y_train, y_test = y_np[:50000], y_np[50000:]\n",
    "x_train, x_test = x_np[:50000, :], x_np[50000:, :]\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(x_train, y_train)\n",
    "y_pred = log_reg.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = torch.randn(100000)\n",
    "x_2 = torch.randn(100000)\n",
    "x_useful = torch.cos(1.5*x_1)*(x_2**2)\n",
    "x_1_rest_small = torch.randn(100000, 15)+ 0.01*x_1.unsqueeze(1)\n",
    "x_1_rest_large = torch.randn(100000, 15) + 0.1*x_1.unsqueeze(1)\n",
    "x_2_rest_small = torch.randn(100000, 15)+ 0.01*x_2.unsqueeze(1)\n",
    "x_2_rest_large = torch.randn(100000, 15) + 0.1*x_2.unsqueeze(1)\n",
    "x = torch.cat([x_1[:, None], x_2[:, None], x_1_rest_small, x_1_rest_large, x_2_rest_small, x_2_rest_large], dim=1)\n",
    "y = ((10*x_useful) + 5*torch.randn(100000) >0.0).type(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "y_train, y_test = y.numpy()[:50000], y.numpy()[50000:]\n",
    "x_train, x_test = x.numpy()[:50000, :], x.numpy()[50000:, :]\n",
    "oracle_train, oracle_test = x_useful.numpy()[:50000], x_useful.numpy()[50000:]\n",
    "log_reg_2 = LogisticRegression()\n",
    "log_reg_2.fit(oracle_train[:, None],y_train)\n",
    "y_pred = log_reg_2.predict(oracle_test[:, None])\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.60666"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, y_test = y.numpy()[:50000], y.numpy()[50000:]\n",
    "x_train, x_test = x.numpy()[:50000, :], x.numpy()[50000:, :]\n",
    "log_reg_3 = LogisticRegression()\n",
    "log_reg_3.fit(x_train, y_train)\n",
    "y_pred = log_reg_3.predict(x_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(100000, 2)\n",
    "noise = torch.randn(100000,)\n",
    "y = ((1.0*x[:,0]+2.0*x[:,1]+noise)>0).type(torch.int64)\n",
    "x_train, x_test = x[:50000, :], x[50000:, :]\n",
    "y_train, y_test = y[:50000], y[50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.len = x.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx, :], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataSet(x_train, y_train)\n",
    "test_dataset = MyDataSet(x_test, y_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 128, shuffle=True, num_workers=6)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 128, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def mish(input):\n",
    "\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Init method.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Forward pass of the function.\n",
    "        '''\n",
    "        return mish(input)\n",
    "\n",
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, res_coef = 0, dropout_p = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear  = nn.Linear(dim_in, dim_out)\n",
    "        self.res_coef = res_coef\n",
    "        self.activation = Mish()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.ln = nn.LayerNorm(dim_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.dropout(y)\n",
    "        if self.res_coef == 0:\n",
    "            return self.ln(y)\n",
    "        else:\n",
    "            return self.ln(self.res_coef*x +y )\n",
    "\n",
    "       \n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, dim_in, dim, res_coef=0.5, dropout_p = 0.1, n_layers = 10):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.ModuleList()\n",
    "        self.first_linear = MLPLayer(dim_in, dim)\n",
    "        self.n_layers = n_layers\n",
    "        for i in range(n_layers):\n",
    "            self.mlp.append(MLPLayer(dim, dim, res_coef, dropout_p))\n",
    "        self.final = nn.Linear(dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        for layer in self.mlp:\n",
    "            x = layer(x)\n",
    "        x= self.sigmoid(self.final(x))\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics import Accuracy\n",
    "class TrainingModule(pl.LightningModule):\n",
    "    def __init__(self, dim_in, dim, res_coef=0, dropout_p=0, n_layers=10):\n",
    "        super().__init__()\n",
    "        self.backbone = MyNetwork(dim_in, dim, res_coef, dropout_p, n_layers)\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.accuracy = Accuracy()\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.backbone(x)\n",
    "        loss = self.loss(x, y.type(torch.float32))\n",
    "        acc = self.accuracy(x, y)\n",
    "        self.log(\"Validation loss\", loss)\n",
    "        self.log(\"Validation acc\", acc)\n",
    "        return loss, acc\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.backbone(x)\n",
    "        loss = self.loss(x, y.type(torch.float32))\n",
    "        acc = self.accuracy(x, y)\n",
    "        self.log(\"Training loss\", loss)\n",
    "        self.log(\"Training acc\", acc)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "import os\n",
    "class CheckpointEveryNSteps(pl.Callback):\n",
    "    def __init__(self, save_step_frequency):\n",
    "        self.save_step_frequency = save_step_frequency\n",
    "\n",
    "    def on_batch_end(self, trainer: pl.Trainer, _):\n",
    "        epoch = trainer.current_epoch\n",
    "        global_step = trainer.global_step\n",
    "        if global_step % self.save_step_frequency == 0:\n",
    "            filename = \"epoch=\" + str(epoch) + \"_step=\" + str(global_step)+\".ckpt\"\n",
    "            ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)\n",
    "            trainer.save_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type      | Params\n",
      "---------------------------------------\n",
      "0 | backbone | MyNetwork | 321   \n",
      "1 | loss     | BCELoss   | 0     \n",
      "2 | accuracy | Accuracy  | 0     \n",
      "---------------------------------------\n",
      "321       Trainable params\n",
      "0         Non-trainable params\n",
      "321       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   5%|▌         | 100/1955 [00:01<00:25, 72.71it/s]   \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  10%|█         | 200/1955 [00:02<00:20, 87.49it/s]/s]\u001b[A\n",
      "Epoch 0:  15%|█▌        | 300/1955 [00:02<00:14, 112.46it/s]s]\u001b[A\n",
      "Epoch 0:  20%|██        | 400/1955 [00:03<00:11, 130.72it/s]s]\u001b[A\n",
      "Epoch 0:  26%|██▌       | 500/1955 [00:03<00:10, 139.97it/s, loss=0.444, v_num=0]\n",
      "Epoch 0:  36%|███▌      | 700/1955 [00:04<00:07, 159.18it/s, loss=0.439, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  41%|████      | 800/1955 [00:05<00:07, 149.34it/s, loss=0.439, v_num=0]\n",
      "Epoch 0:  46%|████▌     | 900/1955 [00:05<00:06, 156.29it/s, loss=0.439, v_num=0]\n",
      "Epoch 0:  51%|█████     | 1000/1955 [00:06<00:05, 162.54it/s, loss=0.439, v_num=0]\n",
      "Epoch 0:  56%|█████▋    | 1100/1955 [00:06<00:05, 164.96it/s, loss=0.352, v_num=0]\n",
      "Epoch 0:  66%|██████▋   | 1300/1955 [00:07<00:03, 173.68it/s, loss=0.358, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  72%|███████▏  | 1400/1955 [00:08<00:03, 166.54it/s, loss=0.358, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 1500/1955 [00:08<00:02, 170.63it/s, loss=0.358, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 1600/1955 [00:09<00:02, 174.35it/s, loss=0.358, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 1700/1955 [00:09<00:01, 175.67it/s, loss=0.34, v_num=0] \n",
      "Epoch 0:  97%|█████████▋| 1900/1955 [00:10<00:00, 181.09it/s, loss=0.344, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  26%|██▌       | 100/391 [00:00<00:02, 108.96it/s]\u001b[A\n",
      "Validating:  51%|█████     | 200/391 [00:01<00:01, 131.83it/s]\u001b[A\n",
      "Validating:  77%|███████▋  | 300/391 [00:01<00:00, 154.60it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 1955/1955 [00:12<00:00, 154.13it/s, loss=0.339, v_num=0]\n",
      "Epoch 1:   5%|▌         | 100/1955 [00:01<00:25, 73.08it/s, loss=0.339, v_num=0]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  10%|█         | 200/1955 [00:02<00:20, 86.36it/s, loss=0.339, v_num=0]\n",
      "Epoch 1:  15%|█▌        | 300/1955 [00:02<00:14, 110.73it/s, loss=0.339, v_num=0]\n",
      "Epoch 1:  20%|██        | 400/1955 [00:03<00:12, 129.05it/s, loss=0.339, v_num=0]\n",
      "Epoch 1:  26%|██▌       | 500/1955 [00:03<00:10, 138.83it/s, loss=0.307, v_num=0]\n",
      "Epoch 1:  36%|███▌      | 700/1955 [00:04<00:07, 158.27it/s, loss=0.309, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  41%|████      | 800/1955 [00:05<00:07, 149.38it/s, loss=0.309, v_num=0]\n",
      "Epoch 1:  46%|████▌     | 900/1955 [00:05<00:06, 156.37it/s, loss=0.309, v_num=0]\n",
      "Epoch 1:  51%|█████     | 1000/1955 [00:06<00:05, 162.83it/s, loss=0.309, v_num=0]\n",
      "Epoch 1:  56%|█████▋    | 1100/1955 [00:06<00:05, 165.13it/s, loss=0.322, v_num=0]\n",
      "Epoch 1:  66%|██████▋   | 1300/1955 [00:07<00:03, 173.67it/s, loss=0.318, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  72%|███████▏  | 1400/1955 [00:08<00:03, 166.33it/s, loss=0.318, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 1500/1955 [00:08<00:02, 170.29it/s, loss=0.318, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 1600/1955 [00:09<00:02, 173.93it/s, loss=0.318, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 1700/1955 [00:09<00:01, 175.13it/s, loss=0.325, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 1900/1955 [00:10<00:00, 180.27it/s, loss=0.326, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  26%|██▌       | 100/391 [00:00<00:02, 107.70it/s]\u001b[A\n",
      "Validating:  51%|█████     | 200/391 [00:01<00:01, 130.43it/s]\u001b[A\n",
      "Validating:  77%|███████▋  | 300/391 [00:01<00:00, 153.08it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 1955/1955 [00:12<00:00, 153.54it/s, loss=0.29, v_num=0] \n",
      "Epoch 1: 100%|██████████| 1955/1955 [00:12<00:00, 150.82it/s, loss=0.29, v_num=0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
    "save_by_steps = CheckpointEveryNSteps(100)\n",
    "training_module = TrainingModule(2, 10, 0.5, 0.1, 2)\n",
    "trainer = pl.Trainer(max_epochs=2, gpus=1, progress_bar_refresh_rate=100, val_check_interval=0.25, logger=tb_logger)\n",
    "trainer.fit(training_module, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x_1 = torch.randn(100000)\n",
    "x_2 = torch.randn(100000)\n",
    "x_useful = torch.cos(1.5*x_1)*(x_2**2)\n",
    "x_1_rest_small = torch.randn(100000, 15)+ 0.01*x_1.unsqueeze(1)\n",
    "x_1_rest_large = torch.randn(100000, 15) + 0.1*x_1.unsqueeze(1)\n",
    "x_2_rest_small = torch.randn(100000, 15)+ 0.01*x_2.unsqueeze(1)\n",
    "x_2_rest_large = torch.randn(100000, 15) + 0.1*x_2.unsqueeze(1)\n",
    "x = torch.cat([x_1[:, None], x_2[:, None], x_1_rest_small, x_1_rest_large, x_2_rest_small, x_2_rest_large], dim=1)\n",
    "y = ((10*x_useful) + 5*torch.randn(100000) >0.0).type(torch.int64) \n",
    "\n",
    "x_train, x_test = x[:50000, :], x[50000:, :]\n",
    "y_train, y_test = y[:50000], y[50000:]\n",
    "train_dataset = MyDataSet(x_train, y_train)\n",
    "test_dataset = MyDataSet(x_test, y_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 32, num_workers=6)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 128, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type      | Params\n",
      "---------------------------------------\n",
      "0 | backbone | MyNetwork | 24.5 K\n",
      "1 | loss     | BCELoss   | 0     \n",
      "2 | accuracy | Accuracy  | 0     \n",
      "---------------------------------------\n",
      "24.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "24.5 K    Total params\n",
      "0.098     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]Epoch 0:  34%|███▍      | 800/2345 [00:21<00:42, 36.49it/s, loss=0.672, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  38%|███▊      | 900/2345 [00:23<00:37, 38.78it/s, loss=0.672, v_num=1]\n",
      "Epoch 0:  43%|████▎     | 1000/2345 [00:23<00:32, 41.80it/s, loss=0.672, v_num=1]\n",
      "Epoch 0:  47%|████▋     | 1100/2345 [00:24<00:27, 44.65it/s, loss=0.672, v_num=1]\n",
      "Epoch 0:  51%|█████     | 1200/2345 [00:25<00:24, 47.20it/s, loss=0.685, v_num=1]\n",
      "Epoch 0:  90%|████████▉ | 2100/2345 [00:46<00:05, 45.04it/s, loss=0.662, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 2200/2345 [00:47<00:03, 45.93it/s, loss=0.662, v_num=1]\n",
      "Epoch 0:  98%|█████████▊| 2300/2345 [00:48<00:00, 47.32it/s, loss=0.662, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 101.10it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2345/2345 [00:50<00:00, 46.77it/s, loss=0.656, v_num=1]\n",
      "Epoch 1:  34%|███▍      | 800/2345 [00:21<00:42, 36.39it/s, loss=0.675, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  38%|███▊      | 900/2345 [00:23<00:37, 38.67it/s, loss=0.675, v_num=1]\n",
      "Epoch 1:  43%|████▎     | 1000/2345 [00:23<00:32, 41.68it/s, loss=0.675, v_num=1]\n",
      "Epoch 1:  47%|████▋     | 1100/2345 [00:24<00:27, 44.51it/s, loss=0.675, v_num=1]\n",
      "Epoch 1:  51%|█████     | 1200/2345 [00:25<00:24, 47.04it/s, loss=0.675, v_num=1]\n",
      "Epoch 1:  90%|████████▉ | 2100/2345 [00:47<00:05, 44.30it/s, loss=0.661, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  94%|█████████▍| 2200/2345 [00:48<00:03, 45.13it/s, loss=0.661, v_num=1]\n",
      "Epoch 1:  98%|█████████▊| 2300/2345 [00:49<00:00, 46.47it/s, loss=0.661, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 96.35it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2345/2345 [00:51<00:00, 45.91it/s, loss=0.657, v_num=1]\n",
      "Epoch 2:  34%|███▍      | 800/2345 [00:21<00:42, 36.53it/s, loss=0.675, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  38%|███▊      | 900/2345 [00:23<00:37, 38.82it/s, loss=0.675, v_num=1]\n",
      "Epoch 2:  43%|████▎     | 1000/2345 [00:23<00:32, 41.84it/s, loss=0.675, v_num=1]\n",
      "Epoch 2:  47%|████▋     | 1100/2345 [00:24<00:27, 44.68it/s, loss=0.675, v_num=1]\n",
      "Epoch 2:  51%|█████     | 1200/2345 [00:25<00:24, 47.20it/s, loss=0.677, v_num=1]\n",
      "Epoch 2:  90%|████████▉ | 2100/2345 [00:46<00:05, 44.88it/s, loss=0.662, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  94%|█████████▍| 2200/2345 [00:48<00:03, 45.77it/s, loss=0.662, v_num=1]\n",
      "Epoch 2:  98%|█████████▊| 2300/2345 [00:48<00:00, 47.15it/s, loss=0.662, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 101.43it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 2345/2345 [00:50<00:00, 46.65it/s, loss=0.657, v_num=1]\n",
      "Epoch 3:  34%|███▍      | 800/2345 [00:22<00:43, 35.93it/s, loss=0.675, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  38%|███▊      | 900/2345 [00:23<00:37, 38.19it/s, loss=0.675, v_num=1]\n",
      "Epoch 3:  43%|████▎     | 1000/2345 [00:24<00:32, 41.17it/s, loss=0.675, v_num=1]\n",
      "Epoch 3:  47%|████▋     | 1100/2345 [00:25<00:28, 43.97it/s, loss=0.675, v_num=1]\n",
      "Epoch 3:  51%|█████     | 1200/2345 [00:25<00:24, 46.49it/s, loss=0.677, v_num=1]\n",
      "Epoch 3:  90%|████████▉ | 2100/2345 [00:47<00:05, 44.00it/s, loss=0.662, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 2200/2345 [00:49<00:03, 44.88it/s, loss=0.662, v_num=1]\n",
      "Epoch 3:  98%|█████████▊| 2300/2345 [00:49<00:00, 46.25it/s, loss=0.662, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 100.56it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 2345/2345 [00:51<00:00, 45.78it/s, loss=0.656, v_num=1]\n",
      "Epoch 4:  34%|███▍      | 800/2345 [00:21<00:41, 36.84it/s, loss=0.675, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  38%|███▊      | 900/2345 [00:23<00:36, 39.08it/s, loss=0.675, v_num=1]\n",
      "Epoch 4:  43%|████▎     | 1000/2345 [00:23<00:32, 42.02it/s, loss=0.675, v_num=1]\n",
      "Epoch 4:  47%|████▋     | 1100/2345 [00:24<00:27, 44.83it/s, loss=0.675, v_num=1]\n",
      "Epoch 4:  51%|█████     | 1200/2345 [00:25<00:24, 47.30it/s, loss=0.677, v_num=1]\n",
      "Epoch 4:  90%|████████▉ | 2100/2345 [00:46<00:05, 44.77it/s, loss=0.662, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  94%|█████████▍| 2200/2345 [00:48<00:03, 45.19it/s, loss=0.662, v_num=1]\n",
      "Epoch 4:  98%|█████████▊| 2300/2345 [00:49<00:00, 46.57it/s, loss=0.662, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:03<00:01, 81.36it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 2345/2345 [00:50<00:00, 46.08it/s, loss=0.656, v_num=1]\n",
      "Epoch 5:  34%|███▍      | 800/2345 [00:21<00:42, 36.61it/s, loss=0.675, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  38%|███▊      | 900/2345 [00:23<00:37, 38.92it/s, loss=0.675, v_num=1]\n",
      "Epoch 5:  43%|████▎     | 1000/2345 [00:23<00:32, 41.96it/s, loss=0.675, v_num=1]\n",
      "Epoch 5:  47%|████▋     | 1100/2345 [00:24<00:27, 44.83it/s, loss=0.675, v_num=1]\n",
      "Epoch 5:  51%|█████     | 1200/2345 [00:25<00:24, 47.38it/s, loss=0.677, v_num=1]\n",
      "Epoch 5:  90%|████████▉ | 2100/2345 [00:47<00:05, 44.63it/s, loss=0.664, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  94%|█████████▍| 2200/2345 [00:48<00:03, 45.48it/s, loss=0.664, v_num=1]\n",
      "Epoch 5:  98%|█████████▊| 2300/2345 [00:49<00:00, 46.84it/s, loss=0.664, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 98.32it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 2345/2345 [00:50<00:00, 46.31it/s, loss=0.657, v_num=1]\n",
      "Epoch 6:  34%|███▍      | 800/2345 [00:22<00:42, 36.27it/s, loss=0.674, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  38%|███▊      | 900/2345 [00:23<00:37, 38.58it/s, loss=0.674, v_num=1]\n",
      "Epoch 6:  43%|████▎     | 1000/2345 [00:24<00:32, 41.61it/s, loss=0.674, v_num=1]\n",
      "Epoch 6:  47%|████▋     | 1100/2345 [00:24<00:28, 44.46it/s, loss=0.674, v_num=1]\n",
      "Epoch 6:  51%|█████     | 1200/2345 [00:25<00:24, 47.05it/s, loss=0.676, v_num=1]\n",
      "Epoch 6:  90%|████████▉ | 2100/2345 [00:47<00:05, 44.59it/s, loss=0.662, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  94%|█████████▍| 2200/2345 [00:48<00:03, 45.45it/s, loss=0.662, v_num=1]\n",
      "Epoch 6:  98%|█████████▊| 2300/2345 [00:49<00:00, 46.83it/s, loss=0.662, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 100.05it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 2345/2345 [00:50<00:00, 46.30it/s, loss=0.656, v_num=1]\n",
      "Epoch 7:  34%|███▍      | 800/2345 [00:21<00:42, 36.62it/s, loss=0.674, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  38%|███▊      | 900/2345 [00:23<00:37, 38.93it/s, loss=0.674, v_num=1]\n",
      "Epoch 7:  43%|████▎     | 1000/2345 [00:23<00:32, 41.99it/s, loss=0.674, v_num=1]\n",
      "Epoch 7:  47%|████▋     | 1100/2345 [00:24<00:27, 44.86it/s, loss=0.674, v_num=1]\n",
      "Epoch 7:  51%|█████     | 1200/2345 [00:25<00:24, 47.43it/s, loss=0.677, v_num=1]\n",
      "Epoch 7:  90%|████████▉ | 2100/2345 [00:47<00:05, 44.37it/s, loss=0.662, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  94%|█████████▍| 2200/2345 [00:48<00:03, 45.23it/s, loss=0.662, v_num=1]\n",
      "Epoch 7:  98%|█████████▊| 2300/2345 [00:49<00:00, 46.60it/s, loss=0.662, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 98.86it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 2345/2345 [00:50<00:00, 46.07it/s, loss=0.657, v_num=1]\n",
      "Epoch 8:  34%|███▍      | 800/2345 [00:22<00:42, 35.95it/s, loss=0.675, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  38%|███▊      | 900/2345 [00:23<00:37, 38.26it/s, loss=0.675, v_num=1]\n",
      "Epoch 8:  43%|████▎     | 1000/2345 [00:24<00:32, 41.27it/s, loss=0.675, v_num=1]\n",
      "Epoch 8:  47%|████▋     | 1100/2345 [00:24<00:28, 44.12it/s, loss=0.675, v_num=1]\n",
      "Epoch 8:  51%|█████     | 1200/2345 [00:25<00:24, 46.57it/s, loss=0.677, v_num=1]\n",
      "Epoch 8:  90%|████████▉ | 2100/2345 [00:47<00:05, 44.01it/s, loss=0.663, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  94%|█████████▍| 2200/2345 [00:49<00:03, 44.88it/s, loss=0.663, v_num=1]\n",
      "Epoch 8:  98%|█████████▊| 2300/2345 [00:49<00:00, 46.26it/s, loss=0.663, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 100.76it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 2345/2345 [00:51<00:00, 45.79it/s, loss=0.657, v_num=1]\n",
      "Epoch 9:  34%|███▍      | 800/2345 [00:22<00:44, 35.04it/s, loss=0.675, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  38%|███▊      | 900/2345 [00:24<00:38, 37.23it/s, loss=0.675, v_num=1]\n",
      "Epoch 9:  43%|████▎     | 1000/2345 [00:24<00:33, 40.16it/s, loss=0.675, v_num=1]\n",
      "Epoch 9:  47%|████▋     | 1100/2345 [00:25<00:29, 42.92it/s, loss=0.675, v_num=1]\n",
      "Epoch 9:  51%|█████     | 1200/2345 [00:26<00:25, 45.37it/s, loss=0.676, v_num=1]\n",
      "Epoch 9:  90%|████████▉ | 2100/2345 [00:48<00:05, 43.73it/s, loss=0.662, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  94%|█████████▍| 2200/2345 [00:49<00:03, 44.61it/s, loss=0.662, v_num=1]\n",
      "Epoch 9:  98%|█████████▊| 2300/2345 [00:50<00:00, 45.97it/s, loss=0.662, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 100.57it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 2345/2345 [00:51<00:00, 45.49it/s, loss=0.655, v_num=1]\n",
      "Epoch 10:  34%|███▍      | 800/2345 [00:22<00:42, 36.10it/s, loss=0.676, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  38%|███▊      | 900/2345 [00:23<00:37, 38.36it/s, loss=0.676, v_num=1]\n",
      "Epoch 10:  43%|████▎     | 1000/2345 [00:24<00:32, 41.36it/s, loss=0.676, v_num=1]\n",
      "Epoch 10:  47%|████▋     | 1100/2345 [00:24<00:28, 44.18it/s, loss=0.676, v_num=1]\n",
      "Epoch 10:  51%|█████     | 1200/2345 [00:25<00:24, 46.72it/s, loss=0.677, v_num=1]\n",
      "Epoch 10:  90%|████████▉ | 2100/2345 [00:47<00:05, 43.93it/s, loss=0.662, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  94%|█████████▍| 2200/2345 [00:49<00:03, 44.80it/s, loss=0.662, v_num=1]\n",
      "Epoch 10:  98%|█████████▊| 2300/2345 [00:49<00:00, 46.16it/s, loss=0.662, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 100.22it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 2345/2345 [00:51<00:00, 45.73it/s, loss=0.656, v_num=1]\n",
      "Epoch 11:  34%|███▍      | 800/2345 [00:22<00:42, 36.22it/s, loss=0.675, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  38%|███▊      | 900/2345 [00:23<00:37, 38.52it/s, loss=0.675, v_num=1]\n",
      "Epoch 11:  43%|████▎     | 1000/2345 [00:24<00:32, 41.55it/s, loss=0.675, v_num=1]\n",
      "Epoch 11:  47%|████▋     | 1100/2345 [00:24<00:28, 44.42it/s, loss=0.675, v_num=1]\n",
      "Epoch 11:  51%|█████     | 1200/2345 [00:25<00:24, 46.94it/s, loss=0.676, v_num=1]\n",
      "Epoch 11:  90%|████████▉ | 2100/2345 [00:47<00:05, 44.02it/s, loss=0.662, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  94%|█████████▍| 2200/2345 [00:48<00:03, 44.90it/s, loss=0.662, v_num=1]\n",
      "Epoch 11:  98%|█████████▊| 2300/2345 [00:49<00:00, 46.27it/s, loss=0.662, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 100.60it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 2345/2345 [00:51<00:00, 45.78it/s, loss=0.656, v_num=1]\n",
      "Epoch 12:  34%|███▍      | 800/2345 [00:21<00:41, 36.79it/s, loss=0.676, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  38%|███▊      | 900/2345 [00:23<00:36, 39.10it/s, loss=0.676, v_num=1]\n",
      "Epoch 12:  43%|████▎     | 1000/2345 [00:23<00:31, 42.14it/s, loss=0.676, v_num=1]\n",
      "Epoch 12:  47%|████▋     | 1100/2345 [00:24<00:27, 45.01it/s, loss=0.676, v_num=1]\n",
      "Epoch 12:  51%|█████     | 1200/2345 [00:25<00:24, 47.59it/s, loss=0.677, v_num=1]\n",
      "Epoch 12:  90%|████████▉ | 2100/2345 [00:47<00:05, 44.23it/s, loss=0.662, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  94%|█████████▍| 2200/2345 [00:48<00:03, 45.10it/s, loss=0.662, v_num=1]\n",
      "Epoch 12:  98%|█████████▊| 2300/2345 [00:49<00:00, 46.45it/s, loss=0.662, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 99.48it/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 2345/2345 [00:51<00:00, 45.95it/s, loss=0.656, v_num=1]\n",
      "Epoch 13:  34%|███▍      | 800/2345 [00:22<00:42, 36.05it/s, loss=0.676, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  38%|███▊      | 900/2345 [00:23<00:37, 38.30it/s, loss=0.676, v_num=1]\n",
      "Epoch 13:  43%|████▎     | 1000/2345 [00:24<00:32, 41.30it/s, loss=0.676, v_num=1]\n",
      "Epoch 13:  47%|████▋     | 1100/2345 [00:24<00:28, 44.12it/s, loss=0.676, v_num=1]\n",
      "Epoch 13:  51%|█████     | 1200/2345 [00:25<00:24, 46.18it/s, loss=0.676, v_num=1]\n",
      "Epoch 13:  90%|████████▉ | 2100/2345 [00:48<00:05, 43.56it/s, loss=0.663, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  94%|█████████▍| 2200/2345 [00:49<00:03, 44.43it/s, loss=0.663, v_num=1]\n",
      "Epoch 13:  98%|█████████▊| 2300/2345 [00:50<00:00, 45.79it/s, loss=0.663, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 99.95it/s]\u001b[A\n",
      "Epoch 13: 100%|██████████| 2345/2345 [00:51<00:00, 45.31it/s, loss=0.657, v_num=1]\n",
      "Epoch 14:  34%|███▍      | 800/2345 [00:22<00:44, 35.10it/s, loss=0.677, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  38%|███▊      | 900/2345 [00:24<00:38, 37.38it/s, loss=0.677, v_num=1]\n",
      "Epoch 14:  43%|████▎     | 1000/2345 [00:24<00:33, 40.34it/s, loss=0.677, v_num=1]\n",
      "Epoch 14:  47%|████▋     | 1100/2345 [00:25<00:28, 43.13it/s, loss=0.677, v_num=1]\n",
      "Epoch 14:  51%|█████     | 1200/2345 [00:26<00:25, 45.65it/s, loss=0.677, v_num=1]\n",
      "Epoch 14:  90%|████████▉ | 2100/2345 [00:47<00:05, 44.19it/s, loss=0.662, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  94%|█████████▍| 2200/2345 [00:48<00:03, 45.08it/s, loss=0.662, v_num=1]\n",
      "Epoch 14:  98%|█████████▊| 2300/2345 [00:49<00:00, 46.45it/s, loss=0.662, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 101.47it/s]\u001b[A\n",
      "Epoch 14: 100%|██████████| 2345/2345 [00:50<00:00, 45.99it/s, loss=0.656, v_num=1]\n",
      "Epoch 15:  34%|███▍      | 800/2345 [00:22<00:42, 36.30it/s, loss=0.676, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  38%|███▊      | 900/2345 [00:23<00:37, 38.56it/s, loss=0.676, v_num=1]\n",
      "Epoch 15:  43%|████▎     | 1000/2345 [00:24<00:32, 41.55it/s, loss=0.676, v_num=1]\n",
      "Epoch 15:  47%|████▋     | 1100/2345 [00:24<00:28, 44.36it/s, loss=0.676, v_num=1]\n",
      "Epoch 15:  51%|█████     | 1200/2345 [00:25<00:24, 46.85it/s, loss=0.676, v_num=1]\n",
      "Epoch 15:  90%|████████▉ | 2100/2345 [00:47<00:05, 44.07it/s, loss=0.663, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  94%|█████████▍| 2200/2345 [00:48<00:03, 44.93it/s, loss=0.663, v_num=1]\n",
      "Epoch 15:  98%|█████████▊| 2300/2345 [00:49<00:00, 46.29it/s, loss=0.663, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 99.46it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 2345/2345 [00:51<00:00, 45.80it/s, loss=0.657, v_num=1]\n",
      "Epoch 16:  34%|███▍      | 800/2345 [00:22<00:42, 35.99it/s, loss=0.675, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  38%|███▊      | 900/2345 [00:23<00:37, 38.21it/s, loss=0.675, v_num=1]\n",
      "Epoch 16:  43%|████▎     | 1000/2345 [00:24<00:32, 41.19it/s, loss=0.675, v_num=1]\n",
      "Epoch 16:  47%|████▋     | 1100/2345 [00:25<00:28, 43.99it/s, loss=0.675, v_num=1]\n",
      "Epoch 16:  51%|█████     | 1200/2345 [00:25<00:24, 46.48it/s, loss=0.677, v_num=1]\n",
      "Epoch 16:  90%|████████▉ | 2100/2345 [00:48<00:05, 43.37it/s, loss=0.662, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  94%|█████████▍| 2200/2345 [00:49<00:03, 44.22it/s, loss=0.662, v_num=1]\n",
      "Epoch 16:  98%|█████████▊| 2300/2345 [00:50<00:00, 45.57it/s, loss=0.662, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 97.88it/s]\u001b[A\n",
      "Epoch 16: 100%|██████████| 2345/2345 [00:52<00:00, 45.08it/s, loss=0.657, v_num=1]\n",
      "Epoch 17:  34%|███▍      | 800/2345 [00:22<00:43, 35.70it/s, loss=0.675, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  38%|███▊      | 900/2345 [00:23<00:38, 37.91it/s, loss=0.675, v_num=1]\n",
      "Epoch 17:  43%|████▎     | 1000/2345 [00:24<00:32, 40.88it/s, loss=0.675, v_num=1]\n",
      "Epoch 17:  47%|████▋     | 1100/2345 [00:25<00:28, 43.70it/s, loss=0.675, v_num=1]\n",
      "Epoch 17:  51%|█████     | 1200/2345 [00:25<00:24, 46.18it/s, loss=0.677, v_num=1]\n",
      "Epoch 17:  90%|████████▉ | 2100/2345 [00:47<00:05, 43.84it/s, loss=0.662, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  94%|█████████▍| 2200/2345 [00:49<00:03, 44.72it/s, loss=0.662, v_num=1]\n",
      "Epoch 17:  98%|█████████▊| 2300/2345 [00:49<00:00, 46.08it/s, loss=0.662, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 99.75it/s]\u001b[A\n",
      "Epoch 17: 100%|██████████| 2345/2345 [00:51<00:00, 45.56it/s, loss=0.657, v_num=1]\n",
      "Epoch 18:  34%|███▍      | 800/2345 [00:22<00:43, 35.28it/s, loss=0.675, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  38%|███▊      | 900/2345 [00:24<00:38, 37.48it/s, loss=0.675, v_num=1]\n",
      "Epoch 18:  43%|████▎     | 1000/2345 [00:24<00:33, 40.42it/s, loss=0.675, v_num=1]\n",
      "Epoch 18:  47%|████▋     | 1100/2345 [00:25<00:28, 43.23it/s, loss=0.675, v_num=1]\n",
      "Epoch 18:  51%|█████     | 1200/2345 [00:26<00:25, 45.72it/s, loss=0.676, v_num=1]\n",
      "Epoch 18:  90%|████████▉ | 2100/2345 [00:48<00:05, 43.51it/s, loss=0.662, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  94%|█████████▍| 2200/2345 [00:49<00:03, 44.38it/s, loss=0.662, v_num=1]\n",
      "Epoch 18:  98%|█████████▊| 2300/2345 [00:50<00:00, 45.74it/s, loss=0.662, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 100.03it/s]\u001b[A\n",
      "Epoch 18: 100%|██████████| 2345/2345 [00:51<00:00, 45.28it/s, loss=0.657, v_num=1]\n",
      "Epoch 19:  34%|███▍      | 800/2345 [00:21<00:42, 36.53it/s, loss=0.675, v_num=1] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  38%|███▊      | 900/2345 [00:23<00:37, 38.84it/s, loss=0.675, v_num=1]\n",
      "Epoch 19:  43%|████▎     | 1000/2345 [00:23<00:32, 41.88it/s, loss=0.675, v_num=1]\n",
      "Epoch 19:  47%|████▋     | 1100/2345 [00:24<00:27, 44.74it/s, loss=0.675, v_num=1]\n",
      "Epoch 19:  51%|█████     | 1200/2345 [00:25<00:24, 47.28it/s, loss=0.676, v_num=1]\n",
      "Epoch 19:  90%|████████▉ | 2100/2345 [00:47<00:05, 44.13it/s, loss=0.663, v_num=1]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  94%|█████████▍| 2200/2345 [00:48<00:03, 44.98it/s, loss=0.663, v_num=1]\n",
      "Epoch 19:  98%|█████████▊| 2300/2345 [00:49<00:00, 46.33it/s, loss=0.663, v_num=1]\n",
      "Validating:  77%|███████▋  | 300/391 [00:02<00:00, 98.32it/s]\u001b[A\n",
      "Epoch 19: 100%|██████████| 2345/2345 [00:51<00:00, 45.82it/s, loss=0.656, v_num=1]\n",
      "Epoch 19: 100%|██████████| 2345/2345 [00:51<00:00, 45.45it/s, loss=0.656, v_num=1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
    "\n",
    "training_module = TrainingModule(62, 32, 0.5, 0.1, 20)\n",
    "trainer = pl.Trainer(max_epochs=20, gpus=1, progress_bar_refresh_rate=100, val_check_interval=0.5, logger=tb_logger)\n",
    "trainer.fit(training_module, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "x_train_np, x_test_np = x_train.numpy(), x_test.numpy()\n",
    "y_train_np, y_test_np = y_train.numpy(), y_test.numpy()\n",
    "\n",
    "train_dataset = lgb.Dataset(x_train_np, y_train_np)\n",
    "test_dataset = lgb.Dataset(x_test_np, y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\ttraining's binary_error: 0\tvalid_1's binary_error: 0.29084\r"
     ]
    }
   ],
   "source": [
    "params = {'num_leaves': 31, 'objective': 'binary', 'feature_fraction':0.8, 'bagging_fraction':0.8, 'metric':'binary_error'}\n",
    "num_round=2000\n",
    "eval_list = [train_dataset, test_dataset]\n",
    "lgb_model = lgb.train(params, train_dataset, num_round, valid_sets=eval_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding = 'utf-8'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "def encode_label(x):\n",
    "    unique=sorted(list(set([str(item) for item in np.unique(x)])))\n",
    "    kv = {unique[i]: i for i in range(len(unique))}\n",
    "    vfunc = np.vectorize(lambda x: kv[str(x)])\n",
    "    return vfunc(x)\n",
    "\n",
    "def encode_label_mat(x):\n",
    "    _, ncol = x.shape\n",
    "    result = np.empty_like(x, dtype=int)\n",
    "    for col in range(ncol):\n",
    "        result[:,col] = encode_label(x[:, col])\n",
    "    return result\n",
    "\n",
    "def impute_nan(x, method='median'):\n",
    "    _, ncol = x.shape\n",
    "    result = np.empty_like(x)\n",
    "\n",
    "    for col in range(ncol):\n",
    "        if method == 'median':\n",
    "            data = x[:, col]\n",
    "            impute_value = np.median(data[~pd.isnull(data) & (data != np.inf) & (data != -np.inf)])\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        func = np.vectorize(lambda x: impute_value if pd.isnull(x) else x)\n",
    "        result[:, col] = func(x[:, col])\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_uniform_interval(minimum, maximum, nbins):\n",
    "    result = [minimum]\n",
    "    step_size = (float(maximum - minimum)) / nbins\n",
    "    for index in range(nbins - 1):\n",
    "        result.append(minimum + step_size * (index + 1))\n",
    "    result.append(maximum)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_interval_v2(x, sorted_intervals):\n",
    "    if pd.isnull(x):\n",
    "        return -1\n",
    "    if x == np.inf:\n",
    "        return -2\n",
    "    if x == -np.inf:\n",
    "        return -3\n",
    "    interval = 0\n",
    "    found = False\n",
    "    sorted_intervals.append(np.inf)\n",
    "    while not found and interval < len(sorted_intervals) - 1:\n",
    "        if sorted_intervals[interval] <= x < sorted_intervals[interval + 1]:\n",
    "            return interval\n",
    "        else:\n",
    "            interval += 1\n",
    "\n",
    "\n",
    "def get_quantile_interval(data, nbins):\n",
    "    quantiles = get_uniform_interval(0, 1, nbins)\n",
    "    return list(np.quantile(data[(~pd.isnull(data)) & (data != np.inf) & (data != -np.inf)], quantiles))\n",
    "\n",
    "\n",
    "def discretize(x, nbins=20):\n",
    "    nrow, ncol = x.shape\n",
    "    result = np.empty_like(x)\n",
    "    interval_list = list()\n",
    "    for col in range(ncol):\n",
    "        intervals = sorted(list(set(get_quantile_interval(x[:, col], nbins))))\n",
    "        interval_centroid = list()\n",
    "\n",
    "        for i in range(len(intervals) - 1):\n",
    "            interval_centroid.append(0.5 * (intervals[i] + intervals[i + 1]))\n",
    "        func = np.vectorize(lambda x: get_interval_v2(x, intervals))\n",
    "        result[:, col] = encode_label(func(x[:, col]))\n",
    "        interval_list.append(interval_centroid)\n",
    "    return result.astype(np.int64), interval_list\n",
    "\n",
    "def get_var_type(df):\n",
    "    columns = df.columns\n",
    "    continuous_vars = [x for x in columns if x.startswith('continuous_')]\n",
    "    discrete_vars = [x for x in columns if x.startswith('discrete_')]\n",
    "    other_vars = list()\n",
    "    for column in columns:\n",
    "        if column not in continuous_vars and column not in discrete_vars:\n",
    "            other_vars.append(column)\n",
    "    return {'continuous': continuous_vars,\n",
    "            'discrete': discrete_vars,\n",
    "            'other': other_vars}\n",
    "\n",
    "\n",
    "def get_cont_var(df):\n",
    "    var_types = get_var_type(df)\n",
    "    return var_types['continuous']\n",
    "\n",
    "\n",
    "def get_dis_var(df):\n",
    "    var_types = get_var_type(df)\n",
    "    return var_types['discrete']\n",
    "\n",
    "def drop_const_var(data):\n",
    "    result = data.copy(deep=True)\n",
    "    for col in data.columns:\n",
    "        if len(data.loc[~pd.isnull(data[col]), col].unique()) <= 1:\n",
    "            result.drop(columns=col, inplace=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_np, x_test_np = x_train.numpy(), x_test.numpy()\n",
    "y_train_np, y_test_np = y_train.numpy(), y_test.numpy()\n",
    "x = np.concatenate([x_train_np, x_test_np])\n",
    "x_dis, centroids = discretize(x)\n",
    "x_dis_train = x_dis[:50000, :]\n",
    "x_dis_test = x_dis[50000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = torch.from_numpy(x).type(torch.int32) \n",
    "        self.y = torch.from_numpy(y).type(torch.int32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx, :], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting einops\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingFactory(nn.Module):\n",
    "    def __init__(self, x, dim_out):\n",
    "        super().__init__()\n",
    "        self.dim_out = dim_out\n",
    "        self.module_list = nn.ModuleList(\n",
    "            [nn.Embedding(len(set(np.unique(x[:, col]))), dim_out) for col in range(x.shape[1])])\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = [self.module_list[col](x[:, col]).unsqueeze(2) for col in range(x.shape[1])]\n",
    "        return torch.cat(result, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, reduce, repeat\n",
    "x_dis_test.shape\n",
    "train_dataloader = DataLoader(TabDataset(x_dis_train, y_train_np), batch_size = 32, num_workers=6)\n",
    "test_dataloader = DataLoader(TabDataset(x_dis_test, y_test_np), batch_size = 128, num_workers=6)\n",
    "\n",
    "class TrainingModuleV2(pl.LightningModule):\n",
    "    def __init__(self, x, dim_emb, dim_mlp, res_coef=0, dropout_p=0, n_layers=10):\n",
    "        super().__init__()\n",
    "        self.embedding = EmbeddingFactory(x, dim_emb)\n",
    "        self.backbone = MyNetwork(x.shape[1]*dim_emb, dim_mlp, res_coef, dropout_p, n_layers)\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.accuracy = Accuracy()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.embedding(x)\n",
    "        x = rearrange(x, \"b h e -> b (h e)\")\n",
    "        x = self.backbone(x)\n",
    "        loss = self.loss(x, y.type(torch.float32))\n",
    "        acc = self.accuracy(x, y)\n",
    "        self.log(\"Validation loss\", loss)\n",
    "        self.log(\"Validation acc\", acc)\n",
    "        return loss, acc\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.embedding(x)\n",
    "        x = rearrange(x, \"b h e -> b (h e)\")\n",
    "        x = self.backbone(x)\n",
    "        loss = self.loss(x, y.type(torch.float32))\n",
    "        acc = self.accuracy(x, y)\n",
    "        self.log(\"Training loss\", loss)\n",
    "        self.log(\"Training acc\", acc)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | embedding | EmbeddingFactory | 20.8 K\n",
      "1 | backbone  | MyNetwork        | 106 K \n",
      "2 | loss      | BCELoss          | 0     \n",
      "3 | accuracy  | Accuracy         | 0     \n",
      "-----------------------------------------------\n",
      "127 K     Trainable params\n",
      "0         Non-trainable params\n",
      "127 K     Total params\n",
      "0.510     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.IntTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-70e38ce1838f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtraining_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainingModuleV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar_refresh_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_check_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_logger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# dispath `start_training` or `start_testing` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_or_test_or_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_bar_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;31m# set stage for logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_sanity_val_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_sanity_check_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(self, max_batches, on_epoch)\u001b[0m\n\u001b[1;32m    723\u001b[0m                 \u001b[0;31m# lightning module methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evaluation_step_and_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\u001b[0m in \u001b[0;36mevaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mmodel_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# capture any logged information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-07181e71358f>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b h e -> b (h e)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-8104e420a864>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-8104e420a864>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.IntTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
    "training_module = TrainingModuleV2(x_dis, 16, 64, 0.5, 0.1, 10)\n",
    "trainer = pl.Trainer(max_epochs=1, gpus=1, progress_bar_refresh_rate=100, val_check_interval=0.5, logger=tb_logger)\n",
    "trainer.fit(training_module, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\r\n",
      "Requirement already satisfied: einops in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x_1 = torch.randn(100000)\n",
    "x_2 = torch.randn(100000)\n",
    "x_useful = torch.cos(1.5*x_1)*(x_2**2)\n",
    "x_1_rest_small = torch.randn(100000, 15)+ 0.01*x_1.unsqueeze(1)\n",
    "x_1_rest_large = torch.randn(100000, 15) + 0.1*x_1.unsqueeze(1)\n",
    "x_2_rest_small = torch.randn(100000, 15)+ 0.01*x_2.unsqueeze(1)\n",
    "x_2_rest_large = torch.randn(100000, 15) + 0.1*x_2.unsqueeze(1)\n",
    "x = torch.cat([x_1[:, None], x_2[:, None], x_1_rest_small, x_1_rest_large, x_2_rest_small, x_2_rest_large], dim=1)\n",
    "y = ((10*x_useful) + 5*torch.randn(100000) >0.0).type(torch.int64) \n",
    "\n",
    "x_train, x_test = x[:50000, :], x[50000:, :]\n",
    "y_train, y_test = y[:50000], y[50000:]\n",
    "x_train_np, x_test_np = x_train.numpy(), x_test.numpy()\n",
    "y_train_np, y_test_np = y_train.numpy(), y_test.numpy()\n",
    "x = np.concatenate([x_train_np, x_test_np])\n",
    "x_dis, centroids = discretize(x)\n",
    "x_dis_train = x_dis[:50000, :]\n",
    "x_dis_test = x_dis[50000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Other possible implementations:\n",
    "https://github.com/KrisKorrel/sparsemax-pytorch/blob/master/sparsemax.py\n",
    "https://github.com/msobroza/SparsemaxPytorch/blob/master/mnist/sparsemax.py\n",
    "https://github.com/vene/sparse-structured-attention/blob/master/pytorch/torchsparseattn/sparsemax.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# credits to Yandex https://github.com/Qwicen/node/blob/master/lib/nn_utils.py\n",
    "def _make_ix_like(input, dim=0):\n",
    "    d = input.size(dim)\n",
    "    rho = torch.arange(1, d + 1, device=input.device, dtype=input.dtype)\n",
    "    view = [1] * input.dim()\n",
    "    view[0] = -1\n",
    "    return rho.view(view).transpose(0, dim)\n",
    "\n",
    "\n",
    "class SparsemaxFunction(Function):\n",
    "    \"\"\"\n",
    "    An implementation of sparsemax (Martins & Astudillo, 2016). See\n",
    "    :cite:`DBLP:journals/corr/MartinsA16` for detailed description.\n",
    "    By Ben Peters and Vlad Niculae\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, dim=-1):\n",
    "        \"\"\"sparsemax: normalizing sparse transform (a la softmax)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ctx : torch.autograd.function._ContextMethodMixin\n",
    "        input : torch.Tensor\n",
    "            any shape\n",
    "        dim : int\n",
    "            dimension along which to apply sparsemax\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output : torch.Tensor\n",
    "            same shape as input\n",
    "\n",
    "        \"\"\"\n",
    "        ctx.dim = dim\n",
    "        max_val, _ = input.max(dim=dim, keepdim=True)\n",
    "        input -= max_val  # same numerical stability trick as for softmax\n",
    "        tau, supp_size = SparsemaxFunction._threshold_and_support(input, dim=dim)\n",
    "        output = torch.clamp(input - tau, min=0)\n",
    "        ctx.save_for_backward(supp_size, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        supp_size, output = ctx.saved_tensors\n",
    "        dim = ctx.dim\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[output == 0] = 0\n",
    "\n",
    "        v_hat = (grad_input.sum(dim=dim) / supp_size).squeeze()\n",
    "        v_hat = v_hat.unsqueeze(dim)\n",
    "        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)\n",
    "        return grad_input, None\n",
    "\n",
    "    @staticmethod\n",
    "    def _threshold_and_support(input, dim=-1):\n",
    "        \"\"\"Sparsemax building block: compute the threshold\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input: torch.Tensor\n",
    "            any dimension\n",
    "        dim : int\n",
    "            dimension along which to apply the sparsemax\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tau : torch.Tensor\n",
    "            the threshold value\n",
    "        support_size : torch.Tensor\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        input_srt, _ = torch.sort(input, descending=True, dim=dim)\n",
    "        input_cumsum = input_srt.cumsum(dim) - 1\n",
    "        rhos = _make_ix_like(input, dim)\n",
    "        support = rhos * input_srt > input_cumsum\n",
    "\n",
    "        support_size = support.sum(dim=dim).unsqueeze(dim)\n",
    "        tau = input_cumsum.gather(dim, support_size - 1)\n",
    "        tau /= support_size.to(input.dtype)\n",
    "        return tau, support_size\n",
    "\n",
    "\n",
    "sparsemax = SparsemaxFunction.apply\n",
    "\n",
    "\n",
    "class Sparsemax(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=-1):\n",
    "        self.dim = dim\n",
    "        super(Sparsemax, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return sparsemax(input, self.dim)\n",
    "\n",
    "\n",
    "class Entmax15(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super().__init_()\n",
    "        self.dim=dim\n",
    "            \n",
    "    @staticmethod\n",
    "    def _threshold_and_support(input, dim=-1):\n",
    "        Xsrt, _ = torch.sort(input, descending=True, dim=dim)\n",
    "\n",
    "        rho = _make_ix_like(input, dim)\n",
    "        mean = Xsrt.cumsum(dim) / rho\n",
    "        mean_sq = (Xsrt ** 2).cumsum(dim) / rho\n",
    "        ss = rho * (mean_sq - mean ** 2)\n",
    "        delta = (1 - ss) / rho\n",
    "\n",
    "        delta_nz = torch.clamp(delta, 0)\n",
    "        tau = mean - torch.sqrt(delta_nz)\n",
    "\n",
    "        support_size = (tau <= Xsrt).sum(dim).unsqueeze(dim)\n",
    "        tau_star = tau.gather(dim, support_size - 1)\n",
    "        return tau_star, support_size\n",
    "    def forward(self, input):\n",
    "        max_val, _ = input.max(dim=self.dim, keepdim=True)\n",
    "        input = input - max_val  # same numerical stability trick as for softmax\n",
    "        input = input / 2  # divide by 2 to solve actual Entmax\n",
    "\n",
    "        tau_star, _ = Entmax15Function._threshold_and_support(input, self.dim)\n",
    "        output = torch.clamp(input - tau_star, min=0) ** 2\n",
    "        ctx.save_for_backward(output)\n",
    "        return output \n",
    "\n",
    "    def backward(self, output, grad):\n",
    "        Y = output\n",
    "        gppr = Y.sqrt()  # = 1 / g'' (Y)\n",
    "        dX = grad_output * gppr\n",
    "        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)\n",
    "        q = q.unsqueeze(ctx.dim)\n",
    "        dX -= q * gppr\n",
    "        return dX, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, BatchNorm1d, ReLU\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def initialize_non_glu(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim+output_dim)/np.sqrt(4*input_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
    "    # torch.nn.init.zeros_(module.bias)\n",
    "    return\n",
    "\n",
    "\n",
    "def initialize_glu(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim+output_dim)/np.sqrt(input_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
    "    # torch.nn.init.zeros_(module.bias)\n",
    "    return\n",
    "\n",
    "\n",
    "class GBN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Ghost Batch Normalization\n",
    "        https://arxiv.org/abs/1705.08741\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, virtual_batch_size=128, momentum=0.01):\n",
    "        super(GBN, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.bn = BatchNorm1d(self.input_dim, momentum=momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        chunks = x.chunk(int(np.ceil(x.shape[0] / self.virtual_batch_size)), 0)\n",
    "        res = [self.bn(x_) for x_ in chunks]\n",
    "\n",
    "        return torch.cat(res, dim=0)\n",
    "\n",
    "\n",
    "class TabNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim,\n",
    "                 n_d=64, n_a=64,\n",
    "                 n_steps=5, gamma=1.3,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,\n",
    "                 virtual_batch_size=128, momentum=0.02,\n",
    "                 mask_type=\"sparsemax\"):\n",
    "        \"\"\"\n",
    "        Defines main part of the TabNet network without the embedding layers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Number of features\n",
    "        output_dim : int or list of int for multi task classification\n",
    "            Dimension of network output\n",
    "            examples : one for regression, 2 for binary classification etc...\n",
    "        n_d : int\n",
    "            Dimension of the prediction  layer (usually between 4 and 64)\n",
    "        n_a : int\n",
    "            Dimension of the attention  layer (usually between 4 and 64)\n",
    "        n_steps : int\n",
    "            Number of sucessive steps in the newtork (usually betwenn 3 and 10)\n",
    "        gamma : float\n",
    "            Float above 1, scaling factor for attention updates (usually betwenn 1.0 to 2.0)\n",
    "        n_independent : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        n_shared : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        epsilon : float\n",
    "            Avoid log(0), this should be kept very low\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in all batch norm\n",
    "        mask_type : str\n",
    "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
    "        \"\"\"\n",
    "        super(TabNet, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.is_multi_task = isinstance(output_dim, list)\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.mask_type = mask_type\n",
    "        self.initial_bn = BatchNorm1d(self.input_dim, momentum=0.01)\n",
    "\n",
    "        if self.n_shared > 0:\n",
    "            shared_feat_transform = torch.nn.ModuleList()\n",
    "            for i in range(self.n_shared):\n",
    "                if i == 0:\n",
    "                    shared_feat_transform.append(Linear(self.input_dim,\n",
    "                                                        2*(n_d + n_a),\n",
    "                                                        bias=False))\n",
    "                else:\n",
    "                    shared_feat_transform.append(Linear(n_d + n_a, 2*(n_d + n_a), bias=False))\n",
    "\n",
    "        else:\n",
    "            shared_feat_transform = None\n",
    "\n",
    "        self.initial_splitter = FeatTransformer(self.input_dim, n_d+n_a, shared_feat_transform,\n",
    "                                                n_glu_independent=self.n_independent,\n",
    "                                                virtual_batch_size=self.virtual_batch_size,\n",
    "                                                momentum=momentum)\n",
    "\n",
    "        self.feat_transformers = torch.nn.ModuleList()\n",
    "        self.att_transformers = torch.nn.ModuleList()\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            transformer = FeatTransformer(self.input_dim, n_d+n_a, shared_feat_transform,\n",
    "                                          n_glu_independent=self.n_independent,\n",
    "                                          virtual_batch_size=self.virtual_batch_size,\n",
    "                                          momentum=momentum)\n",
    "            attention = AttentiveTransformer(n_a, self.input_dim,\n",
    "                                             virtual_batch_size=self.virtual_batch_size,\n",
    "                                             momentum=momentum,\n",
    "                                             mask_type=self.mask_type)\n",
    "            self.feat_transformers.append(transformer)\n",
    "            self.att_transformers.append(attention)\n",
    "\n",
    "        if self.is_multi_task:\n",
    "            self.multi_task_mappings = torch.nn.ModuleList()\n",
    "            for task_dim in output_dim:\n",
    "                task_mapping = Linear(n_d, task_dim, bias=False)\n",
    "                initialize_non_glu(task_mapping, n_d, task_dim)\n",
    "                self.multi_task_mappings.append(task_mapping)\n",
    "        else:\n",
    "            self.final_mapping = Linear(n_d, output_dim, bias=False)\n",
    "            initialize_non_glu(self.final_mapping, n_d, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = 0\n",
    "        x = self.initial_bn(x)\n",
    "\n",
    "        prior = torch.ones(x.shape, device=x.device)\n",
    "        M_loss = 0\n",
    "        att = self.initial_splitter(x)[:, self.n_d:]\n",
    "\n",
    "        for step in range(self.n_steps):\n",
    "            M = self.att_transformers[step](prior, att)\n",
    "            M_loss += torch.mean(torch.sum(torch.mul(M, torch.log(M+self.epsilon)),\n",
    "                                           dim=1))\n",
    "            # update prior\n",
    "            prior = torch.mul(self.gamma - M, prior)\n",
    "            # output\n",
    "            masked_x = torch.mul(M, x)\n",
    "            out = self.feat_transformers[step](masked_x)\n",
    "            d = ReLU()(out[:, :self.n_d])\n",
    "            res = torch.add(res, d)\n",
    "            # update attention\n",
    "            att = out[:, self.n_d:]\n",
    "\n",
    "        M_loss /= self.n_steps\n",
    "\n",
    "        if self.is_multi_task:\n",
    "            # Result will be in list format\n",
    "            out = []\n",
    "            for task_mapping in self.multi_task_mappings:\n",
    "                out.append(task_mapping(res))\n",
    "        else:\n",
    "            out = self.final_mapping(res)\n",
    "        return out, M_loss\n",
    "\n",
    "    def forward_masks(self, x):\n",
    "        x = self.initial_bn(x)\n",
    "\n",
    "        prior = torch.ones(x.shape, device=x.device)\n",
    "        M_explain = torch.zeros(x.shape, device=x.device)\n",
    "        att = self.initial_splitter(x)[:, self.n_d:]\n",
    "        masks = {}\n",
    "\n",
    "        for step in range(self.n_steps):\n",
    "            M = self.att_transformers[step](prior, att)\n",
    "            masks[step] = M\n",
    "            # update prior\n",
    "            prior = torch.mul(self.gamma - M, prior)\n",
    "            # output\n",
    "            masked_x = torch.mul(M, x)\n",
    "            out = self.feat_transformers[step](masked_x)\n",
    "            d = ReLU()(out[:, :self.n_d])\n",
    "            # explain\n",
    "            step_importance = torch.sum(d, dim=1)\n",
    "            M_explain += torch.mul(M, step_importance.unsqueeze(dim=1))\n",
    "            # update attention\n",
    "            att = out[:, self.n_d:]\n",
    "\n",
    "        return M_explain, masks\n",
    "\n",
    "class AttentiveTransformer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim,\n",
    "                 virtual_batch_size=128,\n",
    "                 momentum=0.02,\n",
    "                 mask_type=\"entmax\"):\n",
    "        \"\"\"\n",
    "        Initialize an attention transformer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Input size\n",
    "        output_dim : int\n",
    "            Outpu_size\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
    "        mask_type : str\n",
    "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
    "        \"\"\"\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.fc = Linear(input_dim, output_dim, bias=False)\n",
    "        initialize_non_glu(self.fc, input_dim, output_dim)\n",
    "        self.bn = GBN(output_dim, virtual_batch_size=virtual_batch_size,\n",
    "                      momentum=momentum)\n",
    "\n",
    "        if mask_type == \"sparsemax\":\n",
    "            # Sparsemax\n",
    "            self.selector = Sparsemax(dim=-1)\n",
    "        elif mask_type == \"entmax\":\n",
    "            # Entmax\n",
    "            self.selector = Entmax15(dim=-1)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Please choose either sparsemax\" +\n",
    "                                      \"or entmax as masktype\")\n",
    "\n",
    "    def forward(self, priors, processed_feat):\n",
    "        x = self.fc(processed_feat)\n",
    "        x = self.bn(x)\n",
    "        x = torch.mul(x, priors)\n",
    "        x = self.selector(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeatTransformer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, shared_layers, n_glu_independent,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(FeatTransformer, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize a feature transformer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Input size\n",
    "        output_dim : int\n",
    "            Outpu_size\n",
    "        shared_layers : torch.nn.ModuleList\n",
    "            The shared block that should be common to every step\n",
    "        n_glu_independant : int\n",
    "            Number of independent GLU layers\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization within GLU block(s)\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            'n_glu': n_glu_independent,\n",
    "            'virtual_batch_size': virtual_batch_size,\n",
    "            'momentum': momentum\n",
    "        }\n",
    "\n",
    "        if shared_layers is None:\n",
    "            # no shared layers\n",
    "            self.shared = torch.nn.Identity()\n",
    "            is_first = True\n",
    "        else:\n",
    "            self.shared = GLU_Block(input_dim, output_dim,\n",
    "                                    first=True,\n",
    "                                    shared_layers=shared_layers,\n",
    "                                    n_glu=len(shared_layers),\n",
    "                                    virtual_batch_size=virtual_batch_size,\n",
    "                                    momentum=momentum)\n",
    "            is_first = False\n",
    "\n",
    "        if n_glu_independent == 0:\n",
    "            # no independent layers\n",
    "            self.specifics = torch.nn.Identity()\n",
    "        else:\n",
    "            spec_input_dim = input_dim if is_first else output_dim\n",
    "            self.specifics = GLU_Block(spec_input_dim, output_dim,\n",
    "                                       first=is_first,\n",
    "                                       **params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        x = self.specifics(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLU_Block(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Independant GLU block, specific to each step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, n_glu=2, first=False, shared_layers=None,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(GLU_Block, self).__init__()\n",
    "        self.first = first\n",
    "        self.shared_layers = shared_layers\n",
    "        self.n_glu = n_glu\n",
    "        self.glu_layers = torch.nn.ModuleList()\n",
    "\n",
    "        params = {\n",
    "            'virtual_batch_size': virtual_batch_size,\n",
    "            'momentum': momentum\n",
    "        }\n",
    "\n",
    "        fc = shared_layers[0] if shared_layers else None\n",
    "        self.glu_layers.append(GLU_Layer(input_dim, output_dim,\n",
    "                                         fc=fc,\n",
    "                                         **params))\n",
    "        for glu_id in range(1, self.n_glu):\n",
    "            fc = shared_layers[glu_id] if shared_layers else None\n",
    "            self.glu_layers.append(GLU_Layer(output_dim, output_dim,\n",
    "                                             fc=fc,\n",
    "                                             **params))\n",
    "\n",
    "    def forward(self, x):\n",
    "        scale = math.sqrt(0.5)\n",
    "        if self.first:  # the first layer of the block has no scale multiplication\n",
    "            x = self.glu_layers[0](x)\n",
    "            layers_left = range(1, self.n_glu)\n",
    "        else:\n",
    "            layers_left = range(self.n_glu)\n",
    "\n",
    "        for glu_id in layers_left:\n",
    "            x = torch.add(x, self.glu_layers[glu_id](x))\n",
    "            x = x*scale\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLU_Layer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, fc=None,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(GLU_Layer, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        if fc:\n",
    "            self.fc = fc\n",
    "        else:\n",
    "            self.fc = Linear(input_dim, 2*output_dim, bias=False)\n",
    "        initialize_glu(self.fc, input_dim, 2*output_dim)\n",
    "\n",
    "        self.bn = GBN(2*output_dim, virtual_batch_size=virtual_batch_size,\n",
    "                      momentum=momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        out = torch.mul(x[:, :self.output_dim], torch.sigmoid(x[:, self.output_dim:]))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | embedding | EmbeddingFactory | 41.7 K\n",
      "1 | backbone  | TabNet           | 1.3 M \n",
      "2 | sigmoid   | Sigmoid          | 0     \n",
      "3 | loss      | BCELoss          | 0     \n",
      "4 | accuracy  | Accuracy         | 0     \n",
      "-----------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.277     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]Epoch 0:  34%|███▍      | 800/2345 [00:40<01:17, 19.83it/s, loss=0.679, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  43%|████▎     | 1000/2345 [00:43<00:58, 22.91it/s, loss=0.679, v_num=0]\n",
      "Epoch 0:  47%|████▋     | 1100/2345 [00:45<00:51, 24.41it/s, loss=0.679, v_num=0]\n",
      "Epoch 0:  51%|█████     | 1200/2345 [00:46<00:44, 25.83it/s, loss=0.702, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 2100/2345 [01:28<00:10, 23.79it/s, loss=0.68, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/391 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 2200/2345 [01:30<00:05, 24.37it/s, loss=0.68, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 2300/2345 [01:31<00:01, 25.10it/s, loss=0.68, v_num=0]\n",
      "Validating:  77%|███████▋  | 300/391 [00:04<00:01, 59.86it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2345/2345 [01:34<00:00, 24.84it/s, loss=0.684, v_num=0]\n",
      "Epoch 0: 100%|██████████| 2345/2345 [01:34<00:00, 24.70it/s, loss=0.684, v_num=0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = torch.from_numpy(x).type(torch.int64) \n",
    "        self.y = torch.from_numpy(y).type(torch.float32).squeeze() \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx, :], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "train_dataloader = DataLoader(TabDataset(x_dis_train, y_train_np), batch_size = 32, num_workers=6)\n",
    "test_dataloader = DataLoader(TabDataset(x_dis_test, y_test_np), batch_size = 128, num_workers=6)\n",
    "class TrainingModuleV2(pl.LightningModule):\n",
    "    def __init__(self, x, dim_emb, dim_out, penalty=1e-3, **kwargs):\n",
    "        super().__init__()\n",
    "        self.penalty = penalty\n",
    "        self.embedding = EmbeddingFactory(x, dim_emb)\n",
    "        self.backbone = TabNet(x.shape[1]*dim_emb, dim_out, **kwargs)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.accuracy = Accuracy()\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        return self.backbone(x)\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.embedding(x)\n",
    "        x = rearrange(x, 'b n e -> b (n e)')\n",
    "        x, _ = self.backbone(x)\n",
    "        x = self.sigmoid(x.squeeze())\n",
    "        loss = self.loss(x.squeeze(), y.type(torch.float32))\n",
    "        acc = self.accuracy(x.squeeze(), y.type(torch.int32))\n",
    "        self.log(\"Validation loss\", loss)\n",
    "        self.log(\"Validation acc\", acc)\n",
    "        return loss, acc\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.embedding(x)\n",
    "        x = rearrange(x, 'b n e -> b (n e)')\n",
    "        x, m_loss = self.backbone(x)\n",
    "        x = self.sigmoid(x.squeeze())\n",
    "        loss = self.loss(x, y.type(torch.float32)) - self.penalty*m_loss\n",
    "        acc = self.accuracy(x, y.type(torch.int32))\n",
    "        self.log(\"Training loss\", loss)\n",
    "        self.log(\"Training acc\", acc)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "training_module = TrainingModuleV2(x_dis, 32, 1, n_steps=2, n_independent=4, n_shared=4,)\n",
    "trainer = pl.Trainer(max_epochs=1, gpus=1, progress_bar_refresh_rate=100, val_check_interval=0.5)\n",
    "trainer.fit(training_module, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import numpy as np\n",
    "class EntityEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_level, emdedding_dim, centroid):\n",
    "        super(EntityEmbeddingLayer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_level, emdedding_dim)\n",
    "        self.centroid = torch.tensor(centroid, dtype=torch.float32).detach_()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x = x[:, None]\n",
    "        d = 1.0 / ((x - self.centroid).abs() + EPS)\n",
    "        w = self.softmax(d)\n",
    "        v = torch.mm(w, self.embedding.weight)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = EntityEmbeddingLayer(20, 4, centroids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
